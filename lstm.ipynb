{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1443c92e0a6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "# from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_original = pd.read_csv('data/first_data/target_v1.csv')\n",
    "weather_original = pd.read_csv('data/first_data/weather_v1.csv')\n",
    "hourly_smp_original = pd.read_csv('data/first_data/hourly_smp_v1.csv')\n",
    "\n",
    "target = target_original.copy()\n",
    "weather = weather_original.copy()\n",
    "hourly_smp = hourly_smp_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price_cl_original = pd.read_csv('data/oil/oil_price_cl.csv')\n",
    "oil_price_du_original = pd.read_csv('data/oil/oil_price_du.csv')\n",
    "oil_price_brt_original = pd.read_csv('data/oil/oil_price_brt.csv')\n",
    "\n",
    "oil_price_cl = oil_price_cl_original.copy()\n",
    "oil_price_du = oil_price_du_original.copy()\n",
    "oil_price_brt = oil_price_brt_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smp and oil data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_split(data) :\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['weekday'] = data['date'].dt.weekday\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "\n",
    "    data = data.drop(['date'], axis = 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = date_time_split(target)\n",
    "target = target.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_data(data, start, end) :\n",
    "    \n",
    "    # time data를 만들기 위해 각 데이터별 날짜의 최대 최솟값을 저장해둠\n",
    "    period = pd.date_range(start = start, end = end)\n",
    "    \n",
    "     #time data 생성\n",
    "    time_data = pd.DataFrame({'date' : period})\n",
    "    time_data = date_time_split(time_data)\n",
    "    \n",
    "    return time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oil_data(oil,start, end) :\n",
    "    #oil_price에서 필요한 column(date, price)만을 추출함 \n",
    "    oil = oil.iloc[:, 1:3]\n",
    "    \n",
    "    # target data의 날짜에 맞춰 sampling\n",
    "    # oil의 가격은 3개월 이후의 smp에 영향을 미치므로 2017-11-02 이후의 데이터만을 sampling\n",
    "    oil = oil[(oil['date'] >= start) & (oil['date'] <= end)]\n",
    "    \n",
    "    #날짜 순서대로 data를 sorting\n",
    "    oil.sort_values(by = ['date'], inplace = True)\n",
    "    \n",
    "    #index를 0부터 시작되도록 초기화(후에 data를 merge할때 index가 다르면 error 발생)\n",
    "    oil.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    oil = date_time_split(oil)\n",
    "\n",
    "    return oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_oil(oil_1, oil_2, oil_3, time) :\n",
    "    \n",
    "    #time_data를 기준으로 oil data들을 merge함\n",
    "    # 이때 주말의 oil data들은 존재하지 않으므로 모두 null값으로 들어감\n",
    "    oil = pd.merge(left = time, right = oil_1, how = 'outer')\n",
    "    oil = pd.merge(left = oil, right = oil_2, how = 'outer')\n",
    "    oil = pd.merge(left = oil, right = oil_3, how = 'outer')\n",
    "    \n",
    "    #주말 oil data의 null값을 각 column의 평균으로 채움\n",
    "    fillna = {'du_price' : oil.du_price.mean(), 'cl_price' : oil.cl_price.mean(), 'brt_price' : oil.brt_price.mean()}\n",
    "    oil = oil.fillna(value = fillna)\n",
    "    \n",
    "    #oil 데이터의 날짜정보를 필요없으므로 drop\n",
    "    oil = oil.iloc[ : , -3: ]\n",
    "    \n",
    "    return oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price_cl.columns = ['place', 'date', 'cl_price']\n",
    "oil_price_du.columns = ['place', 'date', 'du_price']\n",
    "oil_price_brt.columns = ['place', 'date', 'brt_price']\n",
    "\n",
    "start = '2017-11-02'\n",
    "end = '2019-11-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_cl = make_oil_data(oil_price_cl, start, end)\n",
    "oil_du = make_oil_data(oil_price_du, start, end)\n",
    "oil_brt = make_oil_data(oil_price_brt, start, end)\n",
    "\n",
    "time = time_data(oil_cl, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil = merge_oil(oil_cl, oil_du, oil_brt, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.concat([target, oil], axis = 1)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_preprocessing(data) :\n",
    "    \n",
    "    #weather data에서 temp와 date를 제외한 모든 column drop\n",
    "    data = data[data['area'] == 884]\n",
    "    data = data.iloc[:, 1:3]\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    data.columns = ['date', 'temp']\n",
    "    \n",
    "    #weather data의 date를 year, month, day, weekday, hour로 분해해줌\n",
    "    data = date_time_split(data)\n",
    "    \n",
    "    #2018년 2월 1일 00시 데이터와 2019년 2월 1일 00시 데이터 누락\n",
    "    #해당 데이터를 채워줌\n",
    "    column = data.columns\n",
    "    \n",
    "    first_data = [round(data[data['month'] ==2]['temp'].mean(), 1) , 2018, 2, 1, 3, 0]\n",
    "    second_data = [round(data[data['month'] ==2]['temp'].mean(), 1), 2019, 2, 1, 4, 0]\n",
    "    time_data = [first_data, second_data]\n",
    "    time_data = pd.DataFrame(time_data, columns = column)\n",
    "    data = pd.concat([data, time_data])\n",
    "    \n",
    "    data.sort_values(by = ['year', 'month', 'day', 'weekday', 'hour'], inplace = True)\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_daily_temp(data, window_size, daily_size) :\n",
    "    \n",
    "    #하루치 데이터마다 sampling하여 dict 자료구조로 저장\n",
    "    weather_dict = {}\n",
    "    for window in range(daily_size) :\n",
    "        weather_dict[window] = data.iloc[window_size*window+1 : window_size*(window+1), : ]\n",
    "        \n",
    "    \n",
    "    #동일한 기간의 날짜정보만 담은 dataframe 만들기\n",
    "    start = '2018-02-01'\n",
    "    end = '2020-01-31'\n",
    "\n",
    "    period = pd.date_range(start = start, end = end)\n",
    "    period = pd.DataFrame({'date' : period})\n",
    "    period = date_time_split(period)\n",
    "    period = period.iloc[ : , :-1]\n",
    "    \n",
    "    #시간별 온도들을 모아 하루치 평균 온도를 계산\n",
    "    mean_temp = [0] * daily_size\n",
    "    for window in range(daily_size) :\n",
    "        mean_temp[window] = round(weather_dict[window]['temp'].mean(),1)\n",
    "    \n",
    "    period['temp'] = mean_temp\n",
    "    \n",
    "    return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_size = 730\n",
    "window_size = 24\n",
    "\n",
    "weather = temp_preprocessing(weather)\n",
    "weather = make_daily_temp(data = weather, window_size = window_size, daily_size = daily_size)\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine both and make train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(left = weather, right = target, on = ['year', 'month', 'day', 'weekday'])\n",
    "\n",
    "time = train['year'].astype(str) + '-' + train['month'].astype(str) + '-' + train['day'].astype(str)\n",
    "train['date'] = pd.to_datetime(time)\n",
    "\n",
    "train = train[['date', 'year', 'month', 'day', 'weekday', 'temp', 'cl_price', 'du_price', 'brt_price', 'smp_min', 'smp_max', 'smp_mean', 'supply']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_mean = train['smp_mean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "sequence_length = seq_len + 1\n",
    "\n",
    "result = []\n",
    "for index in range(len(smp_mean) - sequence_length):\n",
    "    result.append(smp_mean[index: index + sequence_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = []\n",
    "for window in result:\n",
    "    normalized_window = [((float(p) / float(window[0])) - 1) for p in window]\n",
    "    normalized_data.append(normalized_window)\n",
    "    \n",
    "result = np.array(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "row = int(round(result.shape[0] * 0.9))\n",
    "train = result[:row, :]\n",
    "np.random.shuffle(train)\n",
    "\n",
    "x_train = train[:, 1:9]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "y_train = train[:, 11]\n",
    "\n",
    "x_test = result[row:, 1:9]\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "y_test = result[row:, 11]\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(8, return_sequences=True, input_shape=(8, 1)))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "model.summary()\n",
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=10,\n",
    "    epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "\n",
    "fig = plt.figure(facecolor='white', figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(y_test, label='True')\n",
    "ax.plot(pred, label='Prediction')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
